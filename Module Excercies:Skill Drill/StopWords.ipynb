{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StopWords.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP0R6HULatV8",
        "outputId": "1116add4-5c06-4c3a-8777-0ec557a17bd9"
      },
      "source": [
        "!pip3 install pyspark\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 66kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 19.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=b07cc569d4f93cf635fdc96f53ee044bcc0afcff39e0bc6ef724097543e30815\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDQp6lMnauyi",
        "outputId": "42f7e4fa-5b77-407f-9330-e2268dda8c5b"
      },
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.2'\n",
        "spark_version = 'spark-3.1.2'\n",
        "os.environ['spark-3.1.2']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/$spark_version/$spark_version-bin-hadoop2.7.tgz\n",
        "!tar xf $spark_version-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [1 InRelease 14.2 kB/88.7\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [1 InRelease 43.1 kB/88.7 kB 49%] [Waiting for headers\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,184 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [450 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,414 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,770 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,616 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [33.6 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,184 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [906 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [480 kB]\n",
            "Get:25 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.9 kB]\n",
            "Fetched 12.4 MB in 4s (3,175 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECpG5L6Faw8g"
      },
      "source": [
        " # Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"StopWords\").getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5j4h5uYa24A",
        "outputId": "57cc8e3d-8ce4-4ab3-f8e2-62ce327d74b7"
      },
      "source": [
        "#create dataframe\n",
        "#already has a list of words in df. skipping the tokenization step for now. that step usually takes input and seperates it into a list of words. by creating df that already contains list of words, we are replicating this step\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0, [\"Big\", \"data\", \"is\", \"super\", \"powerful\"]),\n",
        "    (1, [\"This\", \"is\", \"going\", \"to\", \"be\", \"epic\"]),\n",
        "], [\"id\", \"raw\"])                              \n",
        "      \n",
        "sentenceData.show(truncate=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------------------+\n",
            "|id |raw                             |\n",
            "+---+--------------------------------+\n",
            "|0  |[Big, data, is, super, powerful]|\n",
            "|1  |[This, is, going, to, be, epic] |\n",
            "+---+--------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKVnkR9UdMAE"
      },
      "source": [
        "# Import stop words library\n",
        "from pyspark.ml.feature import StopWordsRemover"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdRkRILTdMp0"
      },
      "source": [
        "# Run the Remover\n",
        "#takes the input column that will be passed into the function and an output column to add the results\n",
        "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZakKwEWdY68",
        "outputId": "310ee1e9-8651-42c4-8ba6-8208832cf967"
      },
      "source": [
        "#transform and show data\n",
        "remover.transform(sentenceData).show(truncate=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------------------+----------------------------+\n",
            "|id |raw                             |filtered                    |\n",
            "+---+--------------------------------+----------------------------+\n",
            "|0  |[Big, data, is, super, powerful]|[Big, data, super, powerful]|\n",
            "|1  |[This, is, going, to, be, epic] |[going, epic]               |\n",
            "+---+--------------------------------+----------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quZ4dyuAdjX8",
        "outputId": "3f58d54a-dc99-48c8-da8f-e9acd012f706"
      },
      "source": [
        "#skill drill. Combine both tokenizer and StopWordsRemover on a DataFrame that isn't already broken out into a list of words.\n",
        "skilldrillData = spark.createDataFrame([\n",
        "    (0, [\"Today\", \"is\", \"a\", \"good\", \"day\"]),\n",
        "    (1, [\"Make\", \"tomorrow\", \"even\", \"better\"]),\n",
        "], [\"id\", \"raw\"])                              \n",
        "      \n",
        "skilldrillData.show(truncate=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------------------+\n",
            "|id |raw                           |\n",
            "+---+------------------------------+\n",
            "|0  |[Today, is, a, good, day]     |\n",
            "|1  |[Make, tomorrow, even, better]|\n",
            "+---+------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87AQmZvefK8C"
      },
      "source": [
        "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2SGHiGBfkix",
        "outputId": "ac077382-0c9c-4517-b4eb-7e2777576ce3"
      },
      "source": [
        "remover.transform(skilldrillData).show(truncate=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------------------+------------------------------+\n",
            "|id |raw                           |filtered                      |\n",
            "+---+------------------------------+------------------------------+\n",
            "|0  |[Today, is, a, good, day]     |[Today, good, day]            |\n",
            "|1  |[Make, tomorrow, even, better]|[Make, tomorrow, even, better]|\n",
            "+---+------------------------------+------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyxehc_rfmH-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}